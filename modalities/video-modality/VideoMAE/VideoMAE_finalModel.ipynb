{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d674a-ff32-4bcd-842a-ac1f0a5412c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "from functools import partial\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c6286-1610-4346-ab72-e4b6987ae2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db4f66-573b-40ae-a840-942aff375923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX FOR IMPORT ISSUE FROM HERE: https://github.com/xinntao/Real-ESRGAN/issues/768\n",
    "import sys\n",
    "import types\n",
    "from torchvision.transforms.functional import rgb_to_grayscale\n",
    "\n",
    "# Create a module for `torchvision.transforms.functional_tensor`\n",
    "functional_tensor = types.ModuleType(\"torchvision.transforms.functional_tensor\")\n",
    "functional_tensor.rgb_to_grayscale = rgb_to_grayscale\n",
    "\n",
    "# Add this module to sys.modules so other imports can access it\n",
    "sys.modules[\"torchvision.transforms.functional_tensor\"] = functional_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03dad6f-49fd-4934-af17-8e0ee0676466",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_path = \"/home/k/kyparkypar/ondemand/data/sys/myjobs/projects/default/dataset/CMU-MOSI/Raw_reorganised/\"\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0437bb02-2df9-4c47-b5de-082fb42920c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff6537-b5df-49df-a9f2-737a78b7a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_video_file_paths = (\n",
    "    list(dataset_root_path.glob(\"train/*/*.mp4\"))\n",
    "    + list(dataset_root_path.glob(\"valid/*/*.mp4\"))\n",
    "    + list(dataset_root_path.glob(\"test/*/*.mp4\"))\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10d7f1-46cb-4a71-8b56-4ee75d9af373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_video_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3e1db-a4c0-4eb8-888c-a804fc8b341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = sorted({str(path).split(\"/\")[-2] for path in all_video_file_paths})\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed1934-8e87-41c2-83ae-2f263d1aede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"MCG-NJU/videomae-base\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_ckpt)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f073c7-8416-4c59-8692-e298b9eaa58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7607d-7cdb-4f66-9c30-25b0dcd6f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd741ef-dd5c-4cf3-8833-82e4ad43cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 1e-4\n",
    "\n",
    "max_epochs = 5\n",
    "\n",
    "hidden_size = 32\n",
    "dropout_p = 0.1\n",
    "activation_fn = \"tanh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c76ecf-81aa-4a2c-a5cc-88ae8053a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "if activation_fn == 'relu':\n",
    "    activation_function = nn.ReLU()\n",
    "elif activation_fn == 'tanh':\n",
    "    activation_function = nn.Tanh()\n",
    "elif activation_fn == 'No':\n",
    "    activation_function = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8060211b-b3c5-4012-86eb-a6b64e262477",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('dense', nn.Linear(768, hidden_size)),\n",
    "        ('act_func', activation_function),\n",
    "        ('dropout', nn.Dropout(dropout_p)),\n",
    "        ('dense_outp', nn.Linear(hidden_size, model.config.num_labels)),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb277e-f7df-4b53-a709-4d7a41bcf67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfcefd1-e1ca-431b-84a8-39fbf40c5875",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b49620-77fc-45ec-b90f-acae4940e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames_to_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c353f65-10e4-45d1-ad51-bfe1a61c09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e69da1-78ec-4311-b596-4d490701b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train_transform_augm = Compose(\n",
    "#     [\n",
    "#         ApplyTransformToKey(\n",
    "#             key=\"video\",\n",
    "#             transform=Compose(\n",
    "#                 [\n",
    "#                     UniformTemporalSubsample(num_frames_to_sample),\n",
    "#                     Lambda(lambda x: x / 255.0),\n",
    "#                     Normalize(mean, std),\n",
    "#                     RandomShortSideScale(min_size=256, max_size=320),\n",
    "#                     RandomCrop(resize_to),\n",
    "#                     RandomHorizontalFlip(p=0.5),\n",
    "#                 ]\n",
    "#             ),\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "train_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb657bcc-8cba-4644-8942-63a1b83ea7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b7f56-4f44-4792-85f5-328e94098c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pytorchvideo.data.Ucf101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d143b946-4982-4275-9c5b-9fe1bfd6cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorchvideo.data import Ucf101\n",
    "\n",
    "# class LenEnabledUcf101(Ucf101):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         # Cache the dataset length\n",
    "#         self._length = len(self._video_paths)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self._length\n",
    "\n",
    "# # Use the custom subclass\n",
    "# ex_train_dataset = LenEnabledUcf101(\n",
    "#     data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "#     clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "#     decode_audio=False,\n",
    "#     transform=train_transform,\n",
    "# )\n",
    "\n",
    "# # Example usage\n",
    "# print(len(ex_train_dataset))  # Now len() works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb119c58-9b24-43db-b0b8-1a11110bed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorchvideo.data.clip_sampling import ClipSampler, ClipInfo\n",
    "# from typing import Iterator, Dict\n",
    "# import random\n",
    "\n",
    "# class FractionalClipSampler(ClipSampler):\n",
    "#     def __init__(self, base_sampler, fraction=0.2, seed=42):\n",
    "#         \"\"\"\n",
    "#         A fractional clip sampler that keeps only a fraction of the clips\n",
    "#         selected by the base sampler.\n",
    "\n",
    "#         Args:\n",
    "#             base_sampler (ClipSampler): The base sampler to use for generating clips.\n",
    "#             fraction (float): Fraction of clips to keep (e.g., 0.2 for 20%).\n",
    "#             seed (int): Random seed for reproducibility.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.base_sampler = base_sampler\n",
    "#         self.fraction = fraction\n",
    "#         self.random_state = random.Random(seed)\n",
    "\n",
    "#     def __call__(self, last_clip_time: float, video_duration: float, info_dict: Dict) -> Iterator[ClipInfo]:\n",
    "#         \"\"\"\n",
    "#         Called to generate clips.\n",
    "\n",
    "#         Args:\n",
    "#             last_clip_time (float): Start time of the last clip.\n",
    "#             video_duration (float): Duration of the video.\n",
    "#             info_dict (dict): Additional metadata about the video.\n",
    "\n",
    "#         Returns:\n",
    "#             Iterator[ClipInfo]: A generator of ClipInfo objects for the selected clips.\n",
    "#         \"\"\"\n",
    "#         # Generate clips using the base sampler\n",
    "#         for clip_info in self.base_sampler(last_clip_time, video_duration, info_dict):\n",
    "#             # Randomly keep only a fraction of the clips\n",
    "#             if self.random_state.random() <= self.fraction:\n",
    "#                 yield clip_info\n",
    "\n",
    "#     def reset(self):\n",
    "#         \"\"\"Resets the state of the sampler.\"\"\"\n",
    "#         self.base_sampler.reset()\n",
    "\n",
    "\n",
    "# # Use the fractional sampler\n",
    "# fractional_sampler = FractionalClipSampler(\n",
    "#     pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "#     fraction=0.2,\n",
    "# )\n",
    "\n",
    "\n",
    "# train_dataset_subset = pytorchvideo.data.Ucf101(\n",
    "#     data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "#     clip_sampler=fractional_sampler,\n",
    "#     decode_audio=False,\n",
    "#     transform=train_transform,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589cb35c-1888-4702-bc34-c68dae0001b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"valid\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "# from pytorchvideo.data.clip_sampling import UniformClipSampler\n",
    "\n",
    "# # Set stride to a large value (greater than any video length)\n",
    "# clip_sampler = UniformClipSampler(clip_duration=clip_duration, stride=clip_duration)\n",
    "\n",
    "# test_dataset = pytorchvideo.data.Ucf101(\n",
    "#     data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "#     clip_sampler=clip_sampler,\n",
    "#     decode_audio=False,\n",
    "#     transform=val_transform,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcaedc-b569-4ede-9434-248d5d3f4659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f406ba-5a51-4eb8-a06d-3ae1384aae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "def unnormalize_img(img):\n",
    "    \"\"\"Un-normalizes the image pixels.\"\"\"\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "    \"\"\"Prepares a GIF from a video tensor.\n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "        frames.append(frame_unnormalized)\n",
    "    kargs = {\"duration\": 0.25}\n",
    "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "    return filename\n",
    "\n",
    "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799e02d-76fb-45f4-ad4f-3b68d0235bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_video = next(iter(train_dataset))\n",
    "# video_tensor = sample_video[\"video\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d103052c-981e-4eee-a9c5-aef463826071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_gif(video_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24339c-66f8-4698-b75e-15da1b133647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
    "\n",
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "new_model_name = f\"{model_name}-finetuned-cmu-mosi\"\n",
    "num_epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3fc8f5-4903-4b25-b898-42d77ff2669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbc802-7c64-4888-9c19-2a2747b75778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training run with TrainingArguments class      \n",
    "metric_for_best_model = \"loss\"   # Save the model and the metrics of the current model for the best epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/videomae\",\n",
    "    # logging_dir=\"./logs/videomae\",\n",
    "    # report_to=\"tensorboard\",\n",
    "    learning_rate=learning_rate,                   \n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=max_epochs,                   \n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_strategy=\"epoch\",                       \n",
    "    save_strategy=\"epoch\",\n",
    "    # save_total_limit=0,  # Ensure no checkpoints are saved\n",
    "    # eval_steps=1,\n",
    "    # save_steps=1,\n",
    "    weight_decay=weight_decay,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_for_best_model,\n",
    "    remove_unused_columns=False,\n",
    "    # eval_accumulation_steps=eval_accumulation_steps,\n",
    "    # logging_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    lr_scheduler_type=\"constant\",  # Ensures no decay in learning rate\n",
    "    fp16=True,\n",
    "    max_steps=(train_dataset.num_videos // (batch_size * gradient_accumulation_steps)) * max_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b8f84-e193-41bf-b6a0-e7e0acb3dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_wts = np.array([1.16304348, 0.87704918])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f6c49d-b48d-4a94-940e-4b2d6ce6b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Compute custom loss with class weights\n",
    "        weights = torch.tensor(class_wts, dtype=torch.float).to(device)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91334b68-96d4-4490-8b4c-31b4b0f0ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModelEpochCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.best_acc = 0.0\n",
    "        self.best_epoch = None\n",
    "        self.training_metrics = []  # Track training loss at the end of each epoch\n",
    "        self.eval_metrics = []      # Track evaluation loss at the end of each epoch\n",
    "                        \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is not None and metric_for_best_model == \"loss\":\n",
    "            if \"eval_loss\" in metrics and round(state.epoch) > 1:\n",
    "                # print(state.epoch)\n",
    "                # print(round(state.epoch))\n",
    "                self.eval_metrics.append((round(state.epoch), metrics[\"eval_loss\"]))\n",
    "                current_loss = metrics[\"eval_loss\"]\n",
    "                # print(f\"Epoch #{int(state.epoch)} | Validation Loss: {current_loss:.5f} | Validation Accuracy: {metrics['eval_accuracy']:.5f}\")\n",
    "                if current_loss < self.best_loss:\n",
    "                    self.best_loss = current_loss\n",
    "                    self.best_epoch = round(state.epoch)\n",
    "                    self.best_acc = metrics[\"eval_accuracy\"]\n",
    "                    \n",
    "        elif metrics is not None and metric_for_best_model == \"accuracy\":\n",
    "            if \"eval_loss\" in metrics and round(state.epoch) > 1:\n",
    "                self.eval_metrics.append((round(state.epoch), metrics[\"eval_loss\"]))\n",
    "                current_acc = metrics[\"eval_accuracy\"]\n",
    "                # print(f\"Epoch #{int(state.epoch)} | Validation Accuracy: {metrics['eval_accuracy']:.5f} | Validation Loss: {current_loss:.5f}\")\n",
    "                if current_acc > self.best_acc:\n",
    "                    self.best_acc = current_acc\n",
    "                    self.best_epoch = round(state.epoch)\n",
    "                    self.best_loss = metrics[\"eval_loss\"]\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Log training loss at the end of the epoch\n",
    "        if state.log_history:\n",
    "            # Extract the last logged loss\n",
    "            for log in reversed(state.log_history):\n",
    "                if \"loss\" in log:\n",
    "                    self.training_metrics.append((state.epoch, log[\"loss\"]))\n",
    "                    break\n",
    "\n",
    "    # def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "    #     if logs and \"loss\" in logs:\n",
    "    #         self.training_metrics.append((state.epoch, logs[\"loss\"]))\n",
    "\n",
    "best_model_callback = BestModelEpochCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d00c75-cc58-4237-a789-0c9eefae684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcda0ff-198a-4f68-a1e4-c0fec9447287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    resize = Resize(resize_to)  # Ensure consistent size\n",
    "    videos = []\n",
    "    labels = []\n",
    "\n",
    "    for example in examples:\n",
    "        resized_video = resize(example[\"video\"])  # Resize each video\n",
    "        videos.append(resized_video.permute(1, 0, 2, 3))  # Permute dimensions\n",
    "        labels.append(example[\"label\"])\n",
    "\n",
    "    pixel_values = torch.stack(videos)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \n",
    "            \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974ed08-736f-4de7-a420-c8cd613f783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=image_processor,\n",
    "    callbacks=[best_model_callback],  # Not used during CV, only here to find optimal epochs\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18574f7-ba4d-41f8-b47d-ae01c1952220",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c175d-e6a5-46c1-942a-9a2fdbcb5f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extract metrics\n",
    "# epochs, training_losses = zip(*best_model_callback.training_metrics)\n",
    "# eval_epochs, eval_losses = zip(*best_model_callback.eval_metrics)\n",
    "\n",
    "# # Plot learning curves\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(epochs, training_losses, \n",
    "#          label=\"Training Loss\")\n",
    "# plt.plot(eval_epochs, eval_losses, \n",
    "#          label=\"Validation Loss\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.xticks(range(1, max_epochs+1))\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Learning Curve\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570c04e-b583-4d35-b554-54d47ecdfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Optimal Epochs: \", (int(best_model_callback.best_epoch) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f4061-b858-40e9-b8a2-d11eaecd8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0f2bc-dae0-44a6-8a9f-93a0f9138223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30588f-c3db-4677-9e6f-18b708483237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Avg. test loss: \", preds.metrics['test_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4d47d-095a-4af4-bb25-f70b2d684569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_labels = np.argmax(metrics.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f86c5c-f3f3-4cbd-9ea4-1e414989581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c240fd5-91cc-4fc5-832d-280eea503c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbda914-8b14-4fbe-bc00-36e5734a9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds.label_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e114d099-22ef-4213-96b5-d133e0c4bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# print(classification_report(metrics.label_ids, predicted_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6f345-876d-4e35-95b5-b4be6fec544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# confusion_matrix = pd.crosstab(preds.label_ids, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be75b113-8918-4bc0-b740-2ab1057951d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# # Set the size of the figure\n",
    "# plt.figure(figsize=(10, 7))\n",
    "\n",
    "# # Create a heatmap from the confusion matrix\n",
    "# sns.heatmap(confusion_matrix,\n",
    "#             annot=True,\n",
    "#             fmt='d',\n",
    "#             cmap='Blues',\n",
    "#             cbar=True)\n",
    "\n",
    "# # Set titles and labels\n",
    "# plt.title('Fine-Tuned AST (Optimal Parameters) Confusion Matrix')\n",
    "# plt.xlabel('Predicted Labels')\n",
    "# plt.ylabel('True Labels')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a79a31-f684-463a-b0ce-98e59f70d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, video_or_dataset):\n",
    "    \"\"\"\n",
    "    Run inference on either a single video or a dataset of videos.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to use for inference.\n",
    "        video_or_dataset (Union[torch.Tensor, LabeledVideoDataset]): \n",
    "            A single video tensor or a dataset of videos.\n",
    "    \n",
    "    Returns:\n",
    "        Union[torch.Tensor, Tuple[torch.Tensor, List[int]]]: \n",
    "            If a single video tensor is provided, returns logits (torch.Tensor).\n",
    "            If a dataset is provided, returns a tuple containing:\n",
    "            - logits for all videos (torch.Tensor)\n",
    "            - a list of corresponding labels (List[int]).\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Case 1: Single video input\n",
    "    if isinstance(video_or_dataset, torch.Tensor):\n",
    "        # (num_frames, num_channels, height, width) -> (num_channels, num_frames, height, width)\n",
    "        permuted_video = video_or_dataset.permute(1, 0, 2, 3)\n",
    "        inputs = {\"pixel_values\": permuted_video.unsqueeze(0).to(device)}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.logits\n",
    "\n",
    "    # Case 2: Dataset input\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "    dataset_iterator = iter(video_or_dataset)  # Create an iterator for the dataset\n",
    "    i = 0\n",
    "    for i in range(video_or_dataset.num_videos):\n",
    "        sample = next(dataset_iterator)  # Get the next sample\n",
    "        video = sample[\"video\"]\n",
    "        label = sample[\"label\"]  # Extract label\n",
    "        permuted_video = video.permute(1, 0, 2, 3)\n",
    "        inputs = {\"pixel_values\": permuted_video.unsqueeze(0).to(device)}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        logits_list.append(outputs.logits)\n",
    "        labels_list.append(label)  # Append label to labels_list\n",
    "        print(i, \"->\", outputs.logits, \"-\", label)\n",
    "        i += 1\n",
    "    \n",
    "    return torch.cat(logits_list, dim=0), labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b90c1c-29ae-4e11-976d-4f78c0e3fd9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits = run_inference(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aaf930-162c-4905-8561-a758e4f122c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26f195-399b-4aea-aba4-3353ae1e88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels = logits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6be11-5f77-4bd3-907b-5a7f6e475372",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_logits = logits[0]\n",
    "predicted_labels = predicted_logits.argmax(-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f49b50-102f-445a-8462-dd4c5fe91284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75546372-cdc5-4d16-bd4c-a74754a20b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(real_labels, predicted_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f64c292-6003-4e35-8851-b53d99e5b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to tensors\n",
    "predicted_logits_tensor = torch.tensor(predicted_logits, dtype=torch.float).to(device)\n",
    "real_labels_tensor = torch.tensor(real_labels, dtype=torch.long).to(device)\n",
    "\n",
    "# Compute class weights\n",
    "weights = torch.tensor(class_wts, dtype=torch.float).to(device)\n",
    "loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_fct(predicted_logits_tensor.view(-1, predicted_logits_tensor.size(-1)), real_labels_tensor.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7e565-3ae8-45c3-95d9-db3e200c1236",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avg. test loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e9810-ffb0-45ab-aa52-cde7a8a39022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "confusion_matrix = pd.crosstab(real_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3a2f5-6fe4-47f0-b76c-276cb2c33bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Create a heatmap from the confusion matrix\n",
    "sns.heatmap(confusion_matrix,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            cbar=True)\n",
    "\n",
    "# Set titles and labels\n",
    "plt.title('Fine-Tuned VideoMAE (Optimal Parameters) Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f78fb9-749a-440f-8252-0df5b4f4a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_test_video = next(iter(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c0885-5da6-48aa-a71e-8129314d0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd17bf2-5812-4c2c-95ca-7c7f1b65486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_class_idx = logits.argmax(-1).item()\n",
    "# print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e1b9f-94c6-416d-86ea-da08d7566535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Enviroment",
   "language": "python",
   "name": "my_enviroment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
